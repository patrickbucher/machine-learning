\documentclass[a4paper,11pt]{scrartcl}

\usepackage{fontspec}
    \setmainfont{Liberation Serif}
    \setsansfont{Liberation Sans}
    \setmonofont{Liberation Mono}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}

\setlength\parindent{0pt}

\begin{document}

\author{Patrick Bucher}
\title{Machine Learning}
\subtitle{Personal Formula Collection}
\date{\today}
\maketitle

\tableofcontents

\section{Linear Regression}

\subsection{One Variable}

Prediction:

$$ y = h(x) = \theta_0 + \theta_1 x = \theta^T x $$

Cost Function (Squared Error):

$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 $$

Gradient Descent:

$$ \theta = \theta - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)} $$

\subsection{Multiple Variables}

Prediction:

$$ y = h(x) = \theta_0 + \theta_1 x_1 = + \dots + \theta_n x_n = \theta^T x $$

Cost Function:

$$ J(\theta_j) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}_j) - y^{(i)}_j)^2 $$

Gradient Descent:

$$ \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)}_j, j := 0 \dots n $$

An additional feature $x_0=1$ is introduced, so that the vector $x$ becomes $n+1$ dimensional, which simplifies the matrix calculations.

\medskip

Normal Equation:

$$ \theta = (X^T X)^{-1} X^T y $$

Octave (Complexity with $n$ features: $O(n^3)$):

\begin{verbatim}
theta = pinv(X'*X)*X'*y
\end{verbatim}

\subsection{Normalization}

$$ x_i = \frac{x_i - \mu_i}{s_i} $$

Octave:

\begin{verbatim}
X = (X - mean(X)) ./ std(X)
\end{verbatim}

\section{Classification}

Binary Classification: $y \in \{0, 1\}$, where 0 signifies negative or absent, and 1 signifies positive or present.

\subsection{Logistic Regression}

$$ 0 \leq h_{\theta(x)} \leq 1 $$

Sigmoid Activation Function $g$ (with asymptotes at $y$ 0 and 1, to be interpreted as probabilities):

$$ h_{\theta} = g(\theta^{T}x), g(z) = \frac{1}{1 + e^{-z}} $$

$$ h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $$

Cost Function: $-log(h_{\theta}(x))$ for $y=1$ and $-log(1-h_{\theta}(x))$ for $y=0$, combined:

$$ C(h_{\theta}(x),y) = -y \cdot log(h_{\theta}(x)) - (1-y) \cdot log(1-h_{\theta}(x)) $$

$$ J(\theta)=\frac{1}{m} \sum^m_{i=1} C(h_{\theta}(x^{(i)}),y^{(i)}) $$

With maximum likelihood estimation:

$$ J(\theta) = \frac{1}{m} \Bigg[ \sum^m_{i=1} y^{(i)} \cdot log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \cdot log (1 - h_{\theta}(x^{(i)})) \Bigg] $$

Prediction:

$$ h_{\theta}(x) = \frac{1}{1 + e^{-\theta^{T}x}} $$

Gradient Descent (for each $j$ in $\theta$):

$$ \theta_j := \theta_j - \frac{\alpha}{m} \sum^m_{i=1} (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)}_j $$

Vectorized:

$$ \theta := \theta - \frac{\alpha}{m} \sum^m_{i=1} \Bigg[ (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)} \Bigg] $$

$$ \theta := \theta - \frac{\alpha}{m} X^T(g(X\theta)-\vec{y}) $$

\subsubsection{Regularization (Gradient Descent)}

Regularization mitigates the problem of overfitting for higher-order polynomials.

Regularization term (only regularize $\theta_j$ for $j \geq 1$, but not $\theta_0$):

$$ \lambda \sum_{j=1}^m \theta_j^2 $$

Regularized Cost Function:

$$ J(\theta) = \frac{1}{m} \Bigg[ \sum^m_{i=1} y^{(i)} \cdot log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \cdot log (1 - h_{\theta}(x^{(i)})) \Bigg] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$

Regularized Gradient Descent (for $\theta_j$ with $j \geq 1$):

$$ \theta_0 := \theta_0 - \alpha \Bigg[\frac{1}{m} \sum^m_{i=1} (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)}_j \Bigg] $$

$$ \theta_j := \theta_j - \alpha \Bigg[\frac{1}{m} \sum^m_{i=1} \Big( (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)}_j \Big) + \frac{\lambda}{m}\theta_j \Bigg] $$

\subsubsection{Regularization (Normal Equation)}

To regularize using the normal equation, $(n+1)(n+1)$ matrix $L$ with $i$ rows and $j$ columns and the values 1 (for $i=j \land i \geq 1 \land j \geq 1$) and 0 (all the other indices), respectively, has to be created. (This is an identity matrix of size $n+1$ with the value at $(0,0)$ set to 0.)

$$ \theta = (X^TX+\lambda L)^{-1}X^Ty $$

With regularization, the matrix is always inversible.

\section{Neural Networks}

Definitions:

\begin{itemize}
    \item $x_0$: bias unit
    \item $a_i^{(j)}$: activation unit $i$ of layer $j$
    \item $\Theta^{(j)}$: weight matrix between layer $j$ and $j+1$
\end{itemize}

Given layer $j$ with $s_j$ units, and layer $j+1$ with $s_{j+1}$ units, the matrix $\Theta^{(j)}$ has the dimensions $s_{j+1}\times(s_{j}+1)$.

\subsection{Activation}

Neural network with three units in the one hidden layer:

$$ a_1^{(2)} = g(\Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \dots) $$
$$ a_2^{(2)} = g(\Theta^{(2)}_{20}x_0 + \Theta^{(2)}_{21}x_1 + \dots) $$
$$ a_3^{(2)} = g(\Theta^{(3)}_{30}x_0 + \Theta^{(3)}_{31}x_1 + \dots) $$
$$ h_{\Theta}(x) = a_1^3 = g(\Theta^{(2)}_{10}a^{(2)}_0 + \Theta^{(2)}_{11}a^{(2)}_1 + \dots)$$

\subsubsection{Vectorization}

With (forward propagation):

$$ a^{(2)}_1 = g(\Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3) $$

And:

$$ z^{(2)}_1 = \Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3 $$

Follows:

$$ a^{(2)}_1 = g(z^{(2)}_1) $$

So that:

$$ z^{(2)} = \Theta^{(1)} x = \Theta^{(1)} a^{(1)} $$

Output layer:

$$ h_{\Theta} = a^{(3)} = g(z^{(3)}) $$

\subsection{Cost Function}

$$ J(\Theta) = -\frac{1}{m} \Bigg[ \sum^{m}_{i=1} \sum^{K}_{k=1} y^{(i)}_k log(h_{\Theta}(x^{(i)}))_{k} + (1-y^{(i)}_k) log(1-(h_{\Theta}(x^{(i)}))_k  \Bigg] + \frac{\lambda}{2m} \sum^{L-1}_{l=1} \sum^{s_l}_{i=1} \sum^{s_{l+1}}_{j=1} (\Theta^{(l)}_{ji})^2 $$

With $(h_{\Theta}(x))_i$ being the $i^{th}$ output. Note that regularization is \textit{not} added to the bias unit, i.e. only for $j \geq 1$.

\subsection{Forward Propagation}

With a single training example $(x,y)$. The first activation is the input (a bias unit $a^{(1)}_0=1$ must be added before):

$$ a^{(1)} = x $$

The second activation is computed using $\Theta$ and the sigmoid function $g(z)$:

$$ z^{(2)} = \Theta^{(2)} a^{(2)} $$
$$ a^{(2)} = g(z^{(2)}) $$

The bias unit $a^{(2)}_0=1$ must be added again, then the further activations ($l$) are computed:

$$ z^{(l)} = \Theta^{(l)} a^{(l)} $$
$$ a^{(l)} = g(z^{(l)}) $$

Finally, the output (layer $L$) is computed:

$$ z^{(L)} = \Theta^{(L)} a^{(L)} $$
$$ a^{(L)} = g(z^{(L)}) = h_{\Theta}(x)$$

\subsection{Backpropagation}

The $\delta$ for the rightmost layer $L$ is computed as:

$$ \delta^{L} = a^{(L)} - y $$

The further $\delta$ values are computed from right to left, down to $l=2$ (no $\delta$ for the input layer):

$$ \delta^{(l)} = \delta^{(l+1)} \Theta^{(l)} g'(z^{(l)}) $$

With (bias unit included in $a^{(l)}$):

$$ g'(z^{(l)}) = a^{(l)} (1 - a^{(l)}) $$

The $\Delta$ values are computed as ($a^{(l)}$ \textit{without} bias unit):

$$ \Delta^{(l)} = (\delta^{(l+1)})^{T} a^{(l)} $$

Finally, the gradients $D$ for $j \geq 1$ are computed as follows:

$$ D^{(l)}_{ij} = \frac{1}{m}(\Delta^{(l)}_{ij} + \lambda \Theta^{(l)}_{ij}) $$

And without regularization for $j = 0$, respectively:

$$ D^{(l)}_{ij} = \frac{1}{m}(\Delta^{(l)}_{ij} $$

Which is the partial derivative of the cost function:

$$ \frac{\partial}{\partial \Theta^{(l)}_{ij}}J(\Theta) = D^{(l)}_{ij} $$

\subsection{Gradient Checking}

Estimate the derivative of $J(\Theta)$ with $\varepsilon \approx 10^{-4}$ (two-sided difference):

$$ \frac{d}{d\Theta} J(\Theta) \approx \frac{J(\Theta+\varepsilon)-J(\Theta-\varepsilon)}{2\varepsilon} $$

The result should only deviate from the $D$ values by a rounding margin.

\subsection{Random initialization}

When working with neural networks, $\Theta$ must be initialized to a random value symmetrically around $0$. A $(10 \times 11)$ matrix is initialized as follows (Octave):

\begin{verbatim}
init_epsilon = 0.1;
Theta = rand(10,11) * (2 * init_epsilon) - init_epsilon;
\end{verbatim}

\section{Error Metrics}

Confusion Matrix:

\begin{table}[h]
    \centering
    \begin{tabularx}{\linewidth}{c c|c|c|}
        & & \multicolumn{2}{c|}{actual} \\
        & & 1 & 0 \\
        \cline{1-4}
        \multirow{2}{*}{\begin{turn}{90} prediction \end{turn}} & 1 & \makecell{true\\positive} & \makecell{false\\positive} \\
        \cline{2-4}
        & 0 & \makecell{false\\negative} & \makecell{true\\negative} \\
        \cline{1-4}
    \end{tabularx}
\end{table}

Precision ($0 \leq P \leq 1$):

$$ P = \frac{tp}{tp+fp} $$

Recall ($0 \leq R \leq 1$):

$$ R = \frac{tp}{tp+fn} $$

$F_{1}$ Score ($0 \leq F_1 \leq 1$):

$$ F_1 = 2 \frac{PR}{P+R} $$

Some rules of thumb:

\begin{itemize}
    \item A higher classification threshold leads to a higher precision and a lower recall.
    \item A lower classification threshold leads to a lower precision and a higher recall.
    \item Many features can help to lower the bias.
    \item Many training examples can help to lower the variance.
    \item If a human expert can predict $y$ based on $x$, more training data can help.
\end{itemize}

\section{Support Vector Machines}

The prediction yields $0$ and $1$ rather than probabilities. Cost Functions with Safety Margins (\textit{Large Margin Classifier}):

$$ \text{cost}_0(\theta^T x^{(i)}): 1 \quad \text{if} \quad \theta^Tx \leq -1, \quad \text{else} \quad 0 $$
$$ \text{cost}_1(\theta^T x^{(i)}): 1 \quad \text{if} \quad \theta^Tx \geq +1, \quad \text{else} \quad 0 $$

Minimize $\theta$ ($C=\frac{1}{\lambda}$):

$$ \displaystyle\min_{\theta} C \sum_{i=1}^m \Bigg[ y^{(i)} \text{cost}_1(\theta^T x^{(i)}) + (1-y^{(i)}) \text{cost}_0(\theta^T x^{(i)}) \Bigg] + \frac{1}{2} \sum_{i=1}^n \theta_j^{2} $$

\subsection{Kernels}

Calculate features depending on proximity (similarity function) using landmarks ($l^{(i)}=x^{(i)}$) with the \textit{Gaussian kernel} (squared euclidian distance $||x-l^{(i)}||^2$):

$$ f_1 = \text{sim}(x,l^{(i)}) = \text{exp}\Bigg( - \frac{|| x - l^{(i)} ||^2}{2\sigma^2} \Bigg) $$

\subsection{Choice of Parameters}

\begin{itemize}
    \item{$C$}
        \begin{itemize}
            \item large $C$: low bias, high variance (small $\lambda$)
            \item small $C$: high bias, low variance (large $\lambda$)
        \end{itemize}
    \item{$\sigma^2$}
        \begin{itemize}
            \item large $\sigma^2$: high bias, low variance (flat gaussian curve)
            \item small $\sigma^2$: low bias, high variance (abrupt gaussian curve)
        \end{itemize}
\end{itemize}

\section{K-Means}

Input: Training Set ($x^{(i)}, x^{(2)}, \dots, x^{(m)}, x \in \mathbb{R}^n$), number of clusters ($K$); Algorithm:

\begin{enumerate}
    \item initialize centroids $\mu_1,\mu_2,\dots,\mu_K \in \mathbb{R}^n$ (pick random training examples)
    \item for $i=1..m$: set $c^{(i)}$ by proximity to $\mu$ ($\displaystyle\min_{k} || x^{(i)} - \mu_{k} ||$) (assign index of closest centroid)
    \item for $j=1..k$: move $\mu_j$ to mean of $x$s with $c=k$
    \item repeat steps 1 to 3
\end{enumerate}

Repeat the algorithm with different random initializations in order to find a global rather than just a local minimum of the cost function ("Distortion of K-Means Algorithm"):

$$ J(c^{(1)},c^{(2)},\dots,c^{(m)},\mu_1,\mu_2,\dots,\mu_K) = \frac{1}{m} \sum_{i=1}^{m} || x^{(i)} - \mu_{c^{(i)}} ||^2 $$

\section{Principal Component Analysis}

Idea: Reduce input matrix $x \in \mathbb{R}^{n}$ to $z \in \mathbb{R}^{k}$ with $k<n$ to reduce amount of features while retaining as much variance as possible to save storage, memory, processing power and for easier visualization. Algorithm:

\begin{enumerate}
    \item preprocess data: mean normalization and feature scaling: $x_j := \frac{x_j - \mu_j}{\sigma}$
    \item compute covariance matrix $\Sigma=\frac{1}{m} \sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T$ (Octave: \texttt{Sigma=(1/m)*X'*X;})
    \item compute eigenvectors of $\Sigma$ (Octave: \texttt{[U,S,V]=svd(Sigma);})
    \item take first $k$ vector (i.e. columns) of U (Octave: \texttt{Ureduce=U(:,1:k);})
    \item compute $z=U_{\text{reduce}}^T x^{(i)} \in \mathbb{R}^k$ (Compression, Octave: \texttt{z=Ureduce'*X;})
    \item reconstruct $x_{\text{approx}} \in \mathbb{R}^n$ from $z \in \mathbb{R}^k$: $x_{\text{approx}} = U_{\text{reduce}} z \approx x$ (Octave: \texttt{Xapprox=Ureduce*z})
\end{enumerate}

The bias unit $x_0=1$ is omitted.

\subsection{Choosing the Number of Principal Components}

Squared Projection Error:

$$ \frac{1}{m} \sum_{i=1}^{m} || x^{(i)} - x^{(i)}_{\text{approx}} ||^2 $$

Total variation in the data:

$$ \frac{1}{m} \sum_{i=1}^{m} || x^{(i)} ||^2 $$

In order to retain $99\%$ of the variance, choose $k = 1..(n-1)$ to be the smallest value, so that:

$$ \frac{ \frac{1}{m} \sum_{i=1}^{m} || x^{(i)} - x^{(i)}_{\text{approx}} ||^2}{ { \frac{1}{m}} \sum_{i=1}^{m} || x^{(i)} ||^2} \leq 0.01$$

Algorithm (for $k=1..(n-1)$):

\begin{enumerate}
    \item compute $U_{\text{reduce}}, z^{(1)}, \dots, z^{(m)}, x_{\text{approx}}^{(i)}, \dots, x_{\text{approx}}^{(m)}$
    \item compute variance thus retained (see formula above)
    \item finish if variance $\leq$ threshold
\end{enumerate}

In Octave, the \texttt{S} in \texttt{[U,S,V]=svd(Sigma)} is a diagonal matrix that can be used to compute the variance retained:

$$ 1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}} \leq 0.01 \quad \text{or} \quad \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}} \geq 0.99 $$

PCA should not be used to address the issue of overfitting; use regularization instead. PCA should only be introduced if really needed.

\section{Anomaly Detection}

Given a dataset $\{ x^{(1)},x^{(2)},\dots,x^{(m)} \}$, $x_{\text{test}}$ is anomalous if:

$$ p(x_{\text{test}}) < \varepsilon $$

or \textit{not} anomalous (i.e. normal) if:

$$ p(x_{\text{test}}) \geq \varepsilon $$

With $x \sim \mathcal{N}(\mu,\sigma^2)$ (Gaussian):

$$ p(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \text{exp}\Bigg(- \frac{(x-\mu)^2}{2\sigma^2} \Bigg)$$

The parameters $\mu$ and $\sigma^2$ can be guessed from the dataset:

$$ \mu_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)} \quad \sigma^2_j = \frac{1}{m} \sum_{i=1}^{m} (x_j^{(i)} - \mu_j)^2 $$

Given an \textit{unlabeled} traning set $x \in \mathbb{R}^n, x \sim \mathcal{N}(\mu,\sigma^2)$:

$$ p(x) = p(x_1;\mu_1,\sigma_1^2) p(x_2;\mu_2,\sigma_2^2) \dots p(x_n;\mu_n,\sigma_n^2) = \prod_{j=1}^{n} p(x_i;\mu_i,\sigma_i^2) = \prod_{j=1}^{n} \frac{e^{- \frac{(x_j-\mu_j)^2}{2\sigma_j^2}}}{\sqrt{2\pi}\sigma_j} $$

Algorithm:

\begin{enumerate}
    \item choose indicative features
    \item fit parameters $\mu_1,\mu_2,\dots,\mu_n$ and  $\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2$
    \item calculate $p(x)$
    \item mark as anomaly if $p(x) <  \varepsilon$
\end{enumerate}

For the evaluation using labeled training data, move all anomalous ($y=1$) examples to the cross validation and test set; only retain normal examples ($y=0$) in the training set (split usually 60/20/20). Evaluate using precision, recall and F1 Score (accuracy is not indicative due to the skewed distribution of $y$). Consider finding parameter $\varepsilon$ using cross validation (usually $0.05$ or $0.01$).

The features being used should be normal distributed (plot with Octave: \texttt{hist(x, nBins)}). Consider deriving new ($x_3 = \frac{x_1}{x_2}$) or transforming existing features ($x_i = log(x_i + C)$) in order to get normally distributed features.

\subsection{Multivariate Gaussian Distribution}

If single variables do not qualify a training example as an outlier, but only a combination thereof, using a multivariate Gaussian distribution can help to detect those correctly. With $\Sigma$ being the covariance matrix, $|\Sigma|$ its determinant, and $\Sigma^{-1}$ its inverse, the model is defined as:

$$ p(x;\mu,\Sigma) = \frac{1}{\sqrt{2\pi}^n |\Sigma|^{\frac{1}{2}}} \text{exp} \Bigg( - \frac{1}{2} (x-\mu)^T \Sigma^{-1}(x-\mu) \Bigg) $$

If the projection of a two-dimensional univariate Gaussian distribution from the top looks like a circle, a multivariate Gaussian distribution enables to model correlations (elliptic shape denoting the positive or negative correlation).

\section{Recommender Systems}

\subsection{Content-Based Recommenders}

Given a table of movies with ratings by user and categorization:

\begin{figure}[H]
    \centering
    \begin{tabularx}{\linewidth}{l|r r r r|r r}
        \textbf{Movie} & \textbf{Alice} & \textbf{Bob} & \textbf{Carol} & \textbf{Dan} & \textbf{Romance} & \textbf{Action} \\
        \cline{1-7}
        Titanic & 4.5 & 3.5 & 5.0 & 3.0 & 4.5 & 3.0 \\
        Speed & 2.0 & 4.0 & 2.5 & 4.5 & 2.5 & 5.0 \\
        Casablanca & 5.0 & 3.5 & 4.5 & 2.5 & 5.0 & 2.0 \\
        Dawn of the Dead & 1.0 & 5.0 & 2.0 & 4.5 & 1.5 & 5.0 \\
        The Outlaw Josey Wales & 2.5 & 5.0 & ? & ? & 2.5 & 4.5 \\
    \end{tabularx}
\end{figure}

The following matrices can be derived:

$$ Y =
\begin{bmatrix}
    4.5 & 3.5 & 5.0 & 3.0 \\
    2.0 & 4.0 & 2.5 & 4.5 \\
    5.0 & 3.5 & 4.5 & 2.5 \\
    1.0 & 5.0 & 2.0 & 4.5 \\
    2.5 & 5.0 & ? & ? 
\end{bmatrix}
\in \mathbb{R}^{m_{\text{movies}} \times n_{\text{users}}}
\quad
\text{and}
\quad
X =
\begin{bmatrix}
    4.5 & 3.0 \\
    2.5 & 5.0 \\
    5.0 & 2.0 \\
    1.5 & 5.0 \\
    2.5 & 4.5
\end{bmatrix}
\in \mathbb{R}^{m_{\text{movies}} \times k_{\text{genres}}} $$

The matrix $R$ contains the information whether or not a user has rated a movie:

$$ R =
\begin{bmatrix}
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 0 & 0
\end{bmatrix}
\in \mathbb{R}^{m_{\text{movies}} \times n_{\text{users}}}
\quad
\text{and}
\quad
\theta \in \mathbb{R}^{n_{\text{users}} \times K_{\text{genres}}} $$

$\theta$ is randomly initialized and contains the user's genre preferences to be learned.

For each user $j$ and each genre $k$, learn $\theta^{j}$:

$$ \theta^{(j)}_{k} := \theta^{(j)} - \alpha \sum_{i:R(i,j)=1} \Big( (\theta^{(j)})^{T}X^{(i)} - Y^{(i,j)} \Big) X_{k}^{(i)} + \lambda \theta_{k}^{(i)}$$

Cost Function (without regularization):

$$ J = \frac{1}{2} \sum_{j=1}^{n_{\text{users}}} \sum_{i:R(i,j)=1} \Big( (\theta^{(j)})^{T}X^{(i)} - Y^{(i,j)} \Big)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_{\text{users}}} \sum_{k=1}^{K} (\theta_k^{(j)})^2 $$

Notice that only entries with $R(i,j)=1$ must be considered for those calculations.

The missing ratings $R(i,j)=0$ can be predicted as:

$$ X \theta^T $$

\subsection{Collaborative Filtering}

TODO

\end{document}
